%!TEX root = ../dokumentation.tex
\chapter{Conclusion}

The main goal of this research project was to optimize the \acs{ARIMA} training algorithm for Apache SystemML. Or more precisely, to optimize the scoring function that \acs{ARIMA} is based on. In chapter \ref{arimamodel} the mathematical foundations of this function were discussed at length, including a closer look at the system of linear equations that needed to be solved for any process based on the \acl{MA} model. These linear systems were analyzed in more detail in chapter \ref{linsys_solvers} which then also outlined and discussed a number of different approaches that could be used to solve these systems, of which the following three were finally chosen to be implemented: Namely the Jacobi solver, the Forward Substitution solver and the "Inverse" solver.

Chapter \ref{methodology} explained the methods that were to be used to achieve the goal of optimizing \acs{ARIMA} for SystemML. It explained why the scoring function was chosen to be optimized as well as why and how the linear system solvers where chosen. Another part of this chapter was the discussion of the implementation of \acs{ARIMA} for SystemML using the \acl{DML} and how its performance and precision should be measured. Especially the methods of measurement had a significant impact on the final results of this research project. Which is why the first section \ref{conclude_method} of this chapter will be revisiting some of the decision that were made and evaluate them in terms of their effectiveness and efficiency.

The second section \ref{conclude_results} will then proceed with the discussion of the results of the performance and precision tests that were presented previously in chapter \ref{results} and try to explain why the results look the way they do, what might have caused irregularities and what the meaning of the results is with respect to the main goal of this research project. 

And finally, the last section of this chapter will be addressing how the work can be continued as well as how and which parts of this project should be reworked or improved before doing so.

\section{Methodology}\label{conclude_method}

The very first decision that hasn't even been documented as one in the methodology chapter was the decision to use the \acl{DML} to implement the \acs{ARIMA} training algorithm. Of course this was only because that decision had already been made before even starting with the research project. Even though there were good reasons for making this decision (some of them described in the introduction chapter) there were also a number of downsides that started popping up as soon as the implementation work had begun. Especially having to use \acs{DML} had its disadvantages: Debugging became a very painfully process, because of all the code optimizations that were going on under the hood. Additionally the debugger available could only be used from the terminal and had a rather small feature set. And even when used, always having the compiler optimize and restructure the code was making it particularly hard to understand what was happening. Of course one could also try to debug the code using print statements, but even when an error was not occurring before these statements sometimes they still weren't executed. This was a result out of the said code optimizations doing its magic, divided the code into segments which were run separately. And an error in one of those segments would stop all the other instructions in the same segment to be run too. Therefore the error messages were not only vague in the form of "error between lines 120 and 158" but all the print statements used for debugging within these lines were not executed as well and finding the cause of the error was made even more complicated. Of course this also led to the next problem when developing with \acs{DML}. The optimizations were not only impacting the development in a negative manner, but they were also not well documented. And neither were the workarounds that made it possible to "cut" the sections manually into smaller pieces.

Furthermore, because the implementation of R's \textit{stats} package was chosen to be the base line for verifying the correctness of the \acs{ARIMA} implementation, using regression test to make sure that the \acs{DML} implementation was indeed correct turned out to be rather complicated. One had to implement additional scripts only to separately run then read and finally compare the results. And quickly comparing intermediary results was not possible at all without adjusting both scripts. This made developing and debugging the script much harder. To overcome these issues, the script was first developed in native R and only after checking that the script is working properly it was ported to \acs{DML}. Which is a process I would most definitely recommend for any further development using SystemML's \acl{DML}. Even if one does not have to compare it to another R implementation. Being able to use R-Studio as a development environment increased the development speed significantly.

Additionally, the performance test should have been run sightly differently to make it easier to compare the results. The models that were chosen to be tested, were not considered carefully enough and most definitely not deliberately enough. The \acs{AR} and \acs{MA} models should have been chosen in a way that would have allowed comparisons between each one of those models with the full \acs{ARIMA} model. That way one could have analyzed the influence of the \acs{AR} or \acs{MA} part on the \acs{ARIMA} model. But because the models chosen in the form of \acs{AR}(3), \acs{MA}(3) and \acs{ARIMA}(2,1,2), no direct comparison could be done without relying on assumptions about the way an \acs{AR} or \acs{MA} model behaves in terms of execution time when increasing or decreasing the order of the model.



\section{Results}\label{conclude_results}

First and foremost, the results have shown that the implementation of the $\acs{ARIMA}_{CSS}$ function with \acs{DML} is correct and that it produces the exact same results as R's implementation for at least the first 12 decimal places. Of course there were some test runs that completely failed and didn't even produce any result. But first of all, these were only runs for the implementations using the Inverse solver which turned out to be the slowest anyways. And more importantly they were occurring, because the \acl{HDFS} had crashed and SystemML could therefore not read the test data.

With regards to the results of the performance tests for large scale time series there are three important takeaways: 

First of all, \textit{R is generally much faster than DML}, especially when it comes to calculating the $\acs{ARIMA}_{CSS}$ function for a model that needs to solve a system of linear equations. For the class of \acs{AR} models this is not the case, which is one of the factors that allow the \acs{DML} implementation to beat R's performance for these models. But only using the Jacobi solver. Which is particularly interesting considering that for \acs{AR} models there isn't any need for solving a system of linear equations anyways. That we can still see a difference in the performance when using different solvers is due to the fact that there is no check whether we need to solve the linear system or not. The solver function is just called, even if the the coefficient matrix $A$ is the identity matrix $I$ and therefore $b$ would already be equal to the solution vector $x$. For the Jacobi solver this doesn't have a huge impact performance-wise because the it finds the solution immediately in the first iteration. In contrast, the Forward Substitution solver runs the same amount of iterations no matter which values $A$ has and always converges after exactly $n$ iterations for an $n \times n$ coefficient matrix.

Second, when comparing the different solvers among each other, the Jacobi and Forward Substitution solver are ahead with a huge margin, with the Jacobi solver being slightly better than the Forward Substitution solver. But not by much and for the most part the difference isn't even statistically significant and does only appear to be better because of observations that can be clearly defined as outliers. The inverse solver on the other hand is so much slower than the other two, that it wasn't even worth discussing its performance in detail. Using the inverse solver is therefore not recommended and it should be disregarded with completely.

The last takeaway is that there is indeed a clear pattern in the performance data for \acs{MA} and \acs{ARIMA} models. The execution time first starts to rise almost quadratically and then the underlying function suddenly changes and continues in the fashion of a linear model. Unfortunately the reason for this behavior is not known. It might have something to do with the way that SystemML decides at what point the algorithm is distributed on multiple nodes. Which would also explain the weird behavior of \acs{MA}(3) model's execution time that can be seen in figures \ref{fig:ma-comparison-jacobi} and \ref{fig:ma3-exectime-scatter-all_reduced-regression}. In case of the first figure, where \acs{MA}(3) and \acs{MA}(6) are compared, one possible explanation for the rise and fall of \acs{MA}(3)'s execution time could be that the script is first running only on one node, which is taking longer and longer to calculate the solutions for the bigger systems of linear equations, until SystemML's compiler estimates that splitting the computations on two nodes is worth the overhead making the execution time drop again. 

The same explanation could also be applied to the outliers that can be seen in the comparison of the \acl{AR} models. However, the extremely high measurements of the Forward Substitution solver's execution time for \acs{ARIMA}(2,1,2) that were shown in figure \ref{fig:arima212-exectime-scatter-all-regression} can not be explained with this. They are most likely caused by external factors that impacted the cluster on which the tests were running directly.


\section{Next Steps}\label{conclude_whatsnext}

In it self this research project was successfully finished and its goal to optimize the \acs{ARIMA} training algorithm achieved. But obviously there is still room for improvement and further work that can be done to increase the performance of the script even further. A few things that should be improved have already been addressed and the first one that could also quite easily be done is re-running the performance test with a slightly different setup: The orders of \acs{AR}, \acs{MA} and the full \acs{ARIMA} should then be set to the same set of values. And each test configuration should not only be run twice, but five to ten times, to get a more stable test result. With regards to the general set up of the tests there are also some other things that one could decide to invest more time into: For example running the tests with lower values for executor and driver memory of spark to see how the script would behave if it were to max out the memory space and at what point it does this. One could also change the execution mode of SystemML to "single-node" to see how the performance of the scripts would change if SystemML would not distribute the computation over multiple nodes.

Besides fiddling with the settings of the test environment and adding more test configurations or increasing the number of test runs, there's also the question of the optimizer still to be answered. Because even though it was pointed out in chapter \ref{methodology} that optimizing the scoring function promises to give better result, choosing the right optimizer can still have a significant impact on the performance and on the precision of the results. Or even on the question whether a solution will be found at all. Answering this question alone is definitely worth investing a considerable amount of time.

And finally there are also a lot more variations of the \acs{ARIMA} algorithm that could be implemented. There is the seasonal \acl{ARIMA} model as well as the \acl{ARMAX} model and of course one could also add an additional parameter to specify the mean or intercept term or add another parameter for the including a drift term.
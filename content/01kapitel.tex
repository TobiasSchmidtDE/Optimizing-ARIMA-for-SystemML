%!TEX root = ../dokumentation.tex

\chapter{Introduction}

On the 31st of January 2015 \acs{NASA} has launched the \acl{SMAP} (\acs{SMAP}) research satellite to measure land surface soil moisture. The satellite has been established in a near-polar sun-synchronous orbit, repeating its ground track exactly every 8 days. It is equipped with 15.000 sensors that each take an average of 2000 samples per second producing a payload of 4 bytes each. Some of them are used for measuring soil moisture and others are providing vital information to the autopilot or are used to keep track of the health of the satellite and its systems.\textsuperscript{\cite{NASASMAPSpecifications}}
All in all, the data produced amounts to ~10 terabytes \textit{daily}. Over the course of its three-year mission the satellite will gather and return 135 Terabytes of mapping data. And almost all of this data is\textit{ time series data}, meaning each sensor value is associated with the time of its measurement.\textsuperscript{\cite{NASASMAPProducts}} Analyzing this enormous pile of time series data is a great challenge as there is a lack of tools that support both large scale machine learning capabilities as well as machine learning algorithms suited for time series analysis.

There are essentially two parts to the problem that need to be addressed simultaneously. First, we need a way to analyze the time series data itself using appropriate and well-defined models as well as a way to address the sheer amount of data making the training and prediction with these models viable.

The goal of time series analysis on its own is either to understand and interpret the underlying forces that produce the observed system in relation to time or to forecast a future state of the system.\textsuperscript{\cite{Anandh2016EverythingData}} This is done by building a model that represents the data as close as possible while also being as simple as possible. And there are several different kinds of models to choose from. The \acl{ARIMA} (\acs{ARIMA}) model is one of them and wildly spread in terms of practical use. This is mostly because of its ability to take into account a reasonable amount of the complexity that time series data sets can embody. This is mostly due to the fact that the \acs{ARIMA} model is actually a generalization of three of the most common classes of stochastic processes.  The class of \acl{AR} models (\acs{AR}), of integrated (I) models and of \acl{MA} (\acs{MA}) models. 
And of course there are many other models that could address the first issue that this paper tries to solve. However, because of the scientific work already done on \acs{ARIMA} and its status as one of the most basic tools for time series analysis in general, this particular model was chosen as the first part of the solution.

Now, if there wasn't the second part of the problem still left, we would already have more than enough tools offering solutions. Unfortunately, we still need to address the second problem. Only on its own we'd again have multiple solutions ready to go. Essentially the way the "large-scale" issue has been addressed is two-fold: On one hand we have solutions for distributed computing and on the other hand we have solutions for distributed file systems. Solutions for distributed computing address questions on how to divide a given computational task between multiple instances in a way, that results in the task being solved quickly while also making sure that the overall throughput of the cluster, when considering multiple tasks, is maximized. Distributed file systems are then needed to allow a cluster consisting out of multiple servers to share a unified file system. Without this, every instance would need a full copy of all the data that this particular instance currently requires. Resulting in a drastic reduction of overall storage volume while also impacting the distributed computing power. Fortunately for us, there already are multiple frameworks that provide these capabilities. One of them is Apache Hadoop which offers a distributed file system and another one is Apache Spark which can be used for distributed computing.

It might look like we'd already be done here, but solving each one of the problems individually is not the real issue. Solving them simultaneously is. This is where Apache SystemML comes into play. Its main goal, however, is to address a much more deeply grounded issue related to implementing machine learning algorithms like \acs{ARIMA} for large scale systems using Hadoop and Spark. The main issue with that is that data scientists or machine learning engineers, which are meant to develop the machine learning algorithms, are rarely Spark or Hadoop engineers as well and vice versa. To overcome the need of a second engineer that is specialized on cluster computing frameworks like Spark or Hadoop, Apache SystemML has been developed. By providing an R-Like programming language, called \acl{DML} (\acs{DML}), SystemML is able to scale any algorithm based on data and cluster characteristics by automatically compiling the \acs{DML} scripts into a set of Spark \acs{API} commands. This makes the need for a separate Hadoop or Spark engineer obsolete and enables the data scientist to use a familiar programming language while also having an implementation that works for big data.

Therefore the time series analysis model \acs{ARIMA} was implemented for Apache's SystemML using the \acl{DML}. Partially this implementation had already been done before this research project had started. But it had only been fully completed for the \acl{AR} model. Hence, the goal of this research project was to finish the implementation of the full \acl{ARIMA} model and optimize it as much as possible in the set amount of time that was available. As will be discussed in detail in the methodology chapter, there are essentially two parts of the training algorithm, that can be optimized independently. The scoring function of \acs{ARIMA} and the optimizer used to calculate the best weights for the model. For this project, the scoring function was chosen to be optimized first, because it seemed to have more potential for optimization.
